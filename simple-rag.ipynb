{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8beb208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e55f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGSMITH_PROJECT'] = os.getenv(\"LANGSMITH_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6aa0cc",
   "metadata": {},
   "source": [
    "## RAG pipeline starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effd9e4f",
   "metadata": {},
   "source": [
    "### Load the document first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948a7fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2024-10-28T14:17:09+00:00', 'author': 'Denny Lee, Tristen Wentling, Scott Haines, and Prashanth Babu', 'moddate': '2024-10-28T10:38:19-04:00', 'title': 'Delta Lake: The Definitive Guide', 'source': './dldg_databricks.pdf', 'total_pages': 382, 'page': 0, 'page_label': ''}, page_content='Delta Lake \\n  The Definitive Guide\\nModern Data Lakehouse Architectures  \\nwith Data Lakes\\nDenny Lee, Tristen Wentling,  \\nScott Haines & Prashanth Babu\\nForewords by Michael Armbrust \\n& Dominique Brezinski\\nCompliments of')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./dldg_databricks.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b0a40",
   "metadata": {},
   "source": [
    "#### It's the first step is to break the dockuments into smaller chunks as to minimize the token size and the limit of embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720062d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc7d75",
   "metadata": {},
   "source": [
    "##### This step is to calcualte if their is any token size of chunks is greater that the 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debb2889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max-token: 404\n"
     ]
    }
   ],
   "source": [
    "# show the number of tokens for the chunks\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "max_token = 0\n",
    "\n",
    "for i, doc in enumerate(splits):\n",
    "    page_text = doc.page_content\n",
    "    token_count = num_tokens_from_string(page_text, 'cl100k_base')\n",
    "    max_token = max(token_count, max_token)\n",
    "    # print(f\"Page {i+1}: {token_count} tokens\")\n",
    "    \n",
    "print(f'max-token: {max_token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07808eb",
   "metadata": {},
   "source": [
    "#### Create the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb17cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load embeddings model (fits in your 4060 VRAM)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/e5-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52d5b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 4. Create Chroma vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"databricks_docs\",  # optional name\n",
    "    persist_directory=\"./chroma_db\"     # optional, for local persistence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5df53f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 ---\n",
      "Key Features\n",
      "Delta Lake comprises the following key features that are fundamental to an\n",
      "open lakehouse format (please see the VLDB research article “Delta Lake: High-\n",
      "Performance ACID Table Storage over Cloud Object Stores”  for a deeper dive into\n",
      "these features):\n",
      "ACID transactions\n",
      "Delta Lake ensures that data modifications are performed atomically, consis‐\n",
      "tently, in isolation, and durably, i.e., with ACID transaction protections. This\n",
      "means that when multiple concurrent clients or tasks access the data, the system\n",
      "maintains data integrity. For instance, if a process fails during a data modifi‐\n",
      "cation, Delta Lake will roll back the changes, ensuring that the data remains\n",
      "consistent.\n",
      "Scalable metadata\n",
      "The metadata of a Delta Lake table is the transaction log, which provides transac‐\n",
      "tional consistency per the aforementioned ACID transactions. With a petabyte-\n",
      "scale table, the table’s metadata can itself be exceedingly complicated to maintain.\n",
      "\n",
      "--- Result 2 ---\n",
      "Key Features\n",
      "Delta Lake comprises the following key features that are fundamental to an\n",
      "open lakehouse format (please see the VLDB research article “Delta Lake: High-\n",
      "Performance ACID Table Storage over Cloud Object Stores”  for a deeper dive into\n",
      "these features):\n",
      "ACID transactions\n",
      "Delta Lake ensures that data modifications are performed atomically, consis‐\n",
      "tently, in isolation, and durably, i.e., with ACID transaction protections. This\n",
      "means that when multiple concurrent clients or tasks access the data, the system\n",
      "maintains data integrity. For instance, if a process fails during a data modifi‐\n",
      "cation, Delta Lake will roll back the changes, ensuring that the data remains\n",
      "consistent.\n",
      "Scalable metadata\n",
      "The metadata of a Delta Lake table is the transaction log, which provides transac‐\n",
      "tional consistency per the aforementioned ACID transactions. With a petabyte-\n",
      "scale table, the table’s metadata can itself be exceedingly complicated to maintain.\n",
      "\n",
      "--- Result 3 ---\n",
      "Key Features\n",
      "Delta Lake comprises the following key features that are fundamental to an\n",
      "open lakehouse format (please see the VLDB research article “Delta Lake: High-\n",
      "Performance ACID Table Storage over Cloud Object Stores”  for a deeper dive into\n",
      "these features):\n",
      "ACID transactions\n",
      "Delta Lake ensures that data modifications are performed atomically, consis‐\n",
      "tently, in isolation, and durably, i.e., with ACID transaction protections. This\n",
      "means that when multiple concurrent clients or tasks access the data, the system\n",
      "maintains data integrity. For instance, if a process fails during a data modifi‐\n",
      "cation, Delta Lake will roll back the changes, ensuring that the data remains\n",
      "consistent.\n",
      "Scalable metadata\n",
      "The metadata of a Delta Lake table is the transaction log, which provides transac‐\n",
      "tional consistency per the aforementioned ACID transactions. With a petabyte-\n",
      "scale table, the table’s metadata can itself be exceedingly complicated to maintain.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 33, 'title': 'Delta Lake: The Definitive Guide', 'creationdate': '2024-10-28T14:17:09+00:00', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'source': './dldg_databricks.pdf', 'producer': 'Antenna House PDF Output Library 7.1.1639', 'moddate': '2024-10-28T10:38:19-04:00', 'total_pages': 382, 'author': 'Denny Lee, Tristen Wentling, Scott Haines, and Prashanth Babu', 'page_label': '8'}, page_content='Key Features\\nDelta Lake comprises the following key features that are fundamental to an\\nopen lakehouse format (please see the VLDB research article “Delta Lake: High-\\nPerformance ACID Table Storage over Cloud Object Stores”  for a deeper dive into\\nthese features):\\nACID transactions\\nDelta Lake ensures that data modifications are performed atomically, consis‐\\ntently, in isolation, and durably, i.e., with ACID transaction protections. This\\nmeans that when multiple concurrent clients or tasks access the data, the system\\nmaintains data integrity. For instance, if a process fails during a data modifi‐\\ncation, Delta Lake will roll back the changes, ensuring that the data remains\\nconsistent.\\nScalable metadata\\nThe metadata of a Delta Lake table is the transaction log, which provides transac‐\\ntional consistency per the aforementioned ACID transactions. With a petabyte-\\nscale table, the table’s metadata can itself be exceedingly complicated to maintain.'),\n",
       " Document(metadata={'source': './dldg_databricks.pdf', 'author': 'Denny Lee, Tristen Wentling, Scott Haines, and Prashanth Babu', 'title': 'Delta Lake: The Definitive Guide', 'producer': 'Antenna House PDF Output Library 7.1.1639', 'page': 33, 'moddate': '2024-10-28T10:38:19-04:00', 'creationdate': '2024-10-28T14:17:09+00:00', 'total_pages': 382, 'page_label': '8', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)'}, page_content='Key Features\\nDelta Lake comprises the following key features that are fundamental to an\\nopen lakehouse format (please see the VLDB research article “Delta Lake: High-\\nPerformance ACID Table Storage over Cloud Object Stores”  for a deeper dive into\\nthese features):\\nACID transactions\\nDelta Lake ensures that data modifications are performed atomically, consis‐\\ntently, in isolation, and durably, i.e., with ACID transaction protections. This\\nmeans that when multiple concurrent clients or tasks access the data, the system\\nmaintains data integrity. For instance, if a process fails during a data modifi‐\\ncation, Delta Lake will roll back the changes, ensuring that the data remains\\nconsistent.\\nScalable metadata\\nThe metadata of a Delta Lake table is the transaction log, which provides transac‐\\ntional consistency per the aforementioned ACID transactions. With a petabyte-\\nscale table, the table’s metadata can itself be exceedingly complicated to maintain.'),\n",
       " Document(metadata={'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'moddate': '2024-10-28T10:38:19-04:00', 'total_pages': 382, 'producer': 'Antenna House PDF Output Library 7.1.1639', 'title': 'Delta Lake: The Definitive Guide', 'source': './dldg_databricks.pdf', 'page_label': '8', 'creationdate': '2024-10-28T14:17:09+00:00', 'page': 33, 'author': 'Denny Lee, Tristen Wentling, Scott Haines, and Prashanth Babu'}, page_content='Key Features\\nDelta Lake comprises the following key features that are fundamental to an\\nopen lakehouse format (please see the VLDB research article “Delta Lake: High-\\nPerformance ACID Table Storage over Cloud Object Stores”  for a deeper dive into\\nthese features):\\nACID transactions\\nDelta Lake ensures that data modifications are performed atomically, consis‐\\ntently, in isolation, and durably, i.e., with ACID transaction protections. This\\nmeans that when multiple concurrent clients or tasks access the data, the system\\nmaintains data integrity. For instance, if a process fails during a data modifi‐\\ncation, Delta Lake will roll back the changes, ensuring that the data remains\\nconsistent.\\nScalable metadata\\nThe metadata of a Delta Lake table is the transaction log, which provides transac‐\\ntional consistency per the aforementioned ACID transactions. With a petabyte-\\nscale table, the table’s metadata can itself be exceedingly complicated to maintain.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Semantic search function\n",
    "def semantic_search(query: str, k: int = 3):\n",
    "    results = vectorstore.similarity_search(query, k=k)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"\\n--- Result {i} ---\")\n",
    "        print(r.page_content[:10000])  # show first 500 chars\n",
    "    return results\n",
    "\n",
    "# 6. Example query\n",
    "query = \"What are the key features of Delta lakes?\"\n",
    "semantic_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b99120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\nUse the following context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 3}  # top 3 most similar chunks\n",
    ")\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableMap\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e5b4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "# Create a LangChain LLM using Ollama Docker API\n",
    "llm = Ollama(\n",
    "    model=\"gemma2:2b\",         # model inside your Ollama container\n",
    "    base_url=\"http://localhost:11434\",  # Docker port exposed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baedd294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableMap\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langsmith import traceable\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 4️⃣ Runnable RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt                          # format prompt with context + question\n",
    "    | llm                             # generate output using LLaMA\n",
    "    | StrOutputParser()               # parse output to string\n",
    ")\n",
    "\n",
    "\n",
    "answer = rag_chain.invoke(\"What is Delta Lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "980db6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Delta Lake is an open-source storage layer that supports ACID transactions, scalable metadata handling, and unification of streaming and batch data processing. \\n\\nHere's a breakdown from the provided text:\\n\\n* **ACID Transactions:** Ensures the integrity and reliability of your data through consistency, atomicity, isolation, and durability.\\n* **Scalable Metadata Handling:**  Handles large amounts of metadata effectively for your data lake workloads.\\n* **Unification of Streaming and Batch Data Processing:** Allows seamless integration of real-time (streaming) and batch processing operations into a single data platform.\\n\\nEssentially, Delta Lake provides a robust foundation for managing and analyzing your data in a variety of use cases. It's designed to be flexible, allowing you to adapt it to different types of workloads (small, medium, big data).\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53cf7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
